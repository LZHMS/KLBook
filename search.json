[{"title":"蓦然回首 | 藤野 | 京本 | 藤野京","path":"/KLBook/essay/LookBack/","content":"《蓦然回首》晚上有空看了这部前几天刚在腾讯视频上放映的动漫电影，一开始主要是被这种非常温暖和美好的日漫风格所吸引。但看完后，也深有感受，使自己想起来小学六年级时的那段纯真美好的时光，向往而再也不能拥有了。蓦然回首，已成往事。 这部电影是改编自日本漫画家藤本树的同名漫画作品，实际上是藤本树自己对漫画创作不懈追求的真实写照。慕然回首，电影名字英译后，是“Look Back”，一语双关的名字，既是纪念曾经对漫画创作充满热情的那段时光，也是电影中一个四格漫画中的标题，表明不论别人对自己评价怎样，都会继续追寻自己的梦想不断前进。 电影寓意电影寓意颇为丰富，如果单纯从两个少女的共同成长经历来看，更多的是怀念纯真美好的友情，以及对梦想的不懈追求，联系作者本人经历的话，又会给作品增加坚持做回自己的寓意。 最大的感触是那段纯真美好时光的共鸣，两个人相互欣赏，并一起学习、创作漫画。像极了当初自己六年级时一段时光中，与一位好友相互鼓励，共同成长的时光。 一生能有几多挚友？庆幸自己也能有一段短暂而美好，却再也难寻的时光。此后的时间里，再也遇不到能够达到那种高度的情景。 再者便是对童年回忆的共鸣，漫画式的田野是多少人的童年回忆啊，真的很庆幸自己有这种乡村田野间的童年生活，感受到的格外亲切。 片段欣赏学校的漫画周报，同时刊登了藤野和京本的漫画；两种不同风格的漫画，一个侧重内容，一个则侧重背景。 藤野觉得自己被京本打败了，好胜心促使她夜以继日地提升画技。 六年级后期放弃画画的藤野被老师叫去给京本送毕业证书，在京本家中看到了堆叠的大量练习本子。 京本一直以藤野为偶像，两人因对漫画的热爱而相互结识；京本对藤野的漫画表现出认可与期待。 得到认可的京本一路狂奔回去，洋溢着慢慢的喜悦。 漫画式的雨后田间小路，再次唤醒田野里的童年回忆，被这治愈式的风格触动了。 相互欣赏的两人一起追寻着自己的梦想。 两人相互配合不断地画画，作品逐渐多了起来。 共同创作的以两人名字组合——藤野京署名的漫画作品，获得了比赛第二名并有100万日元奖金。 随着后续两人不断连载漫画作品，对漫画充满热情的京本觉得自己目前的速度已经跟不上藤野了，想着去一所美术大学读书。我觉得，京本是希望去大学里面再提高自己的绘画技巧，以便能够更好地帮助藤野，但是由于她性格比较内敛，并没有告诉藤野。 但是，不幸的事情发生了。京本在大学中不幸在一场无差别杀人事件中受害了。 听到消息后的藤野，跑到京本家里面，不断自责着当初给京本的那个最初的四格漫画。 电影也采用了平行宇宙的创作手法，似乎是在设想假设当初两人没有就此认识，京本的命运又该如何。但是可见，京本还是会从事漫画创作，并且去了相同的美术大学，并在那个无差别杀人事件中被学习跆拳道的藤野救下。 此处，从另一个平行宇宙中飘来一张由京本画的四格漫画，标题是《看背后》，藤野逐渐开始明白，因为漫画最后一格上面画着，有一个镰刀插入到背上。寓意着，即使遭受恶意、疼痛的伤痕，也要继续追求自己的梦想并不断坚持下去。 藤野回想着从前一起创作的场景，热情而美好。 为梦想成真不断地努力着，直到最后的成功。 到电影最后，藤野坚定了自己的理想，不再犹豫，还是做回从前那个对漫画充满热情的自己。","tags":["电影","动漫","励志","友情"],"categories":["essay"]},{"title":"Windows 下 3D Vision 环境配置 (Gaussian Avatars)","path":"/KLBook/blog/3DVisionConfig/","content":"GaussianAvatars Repo Installation Requirements tqdmnumpy==1.22.3matplotlibscipychumpytensorboardfvcoreiopathplyfiletyrodearpyguiromagit+https://github.com/NVlabs/nvdiffrast#git+https://github.com/facebookresearch/pytorch3d.gitsubmodules/diff-gaussian-rasterizationsubmodules/simple-knn 项目工具安装Ninja 是一种专注于速度和效率的构建工具，最初由 Google 工程师开发，用于加速大型项目的构建过程。它的设计哲学是通过简洁的构建规则和依赖关系描述来实现快速构建，尤其适合大规模项目和多核处理器环境。Ninja 的构建文件后缀为 .ninja，其语法简单，主要依赖其他工具（如 CMake）生成构建文件。CMake 是一个开源、跨平台的编译、测试和打包工具，用于生成项目的构建文件。它通过简单的语法描述编译和安装过程，并输出适合不同平台的构建文件，如 Makefile 或 Ninja 文件。CMake 的核心配置文件是 CMakeLists.txt，开发者可以通过它定义项目的构建规则。 CMake 生成构建文件：CMake 可以生成多种格式的构建文件，其中包括 Ninja 文件。通过指定 -G Ninja，CMake 会生成 build.ninja 文件。 Ninja 执行构建：生成 build.ninja 文件后，Ninja 负责根据该文件执行实际的构建任务。 优势互补：CMake 提供了强大的跨平台能力和灵活的构建规则描述，而 Ninja 则专注于快速执行构建任务。这种组合使得开发人员可以专注于编写简单的 CMakeLists.txt 文件，同时享受 Ninja 的高速构建能力。 CMakeWindows 平台安装 CMake 相对简单，通过官网下载地址：https://cmake.org/download/ ，选择最新的版本即可下载，安装引导程序中记得选择添加到环境变量配置即可。 测试是否安装成功，以及环境变量是否配置，可以在终端检测 CMake 版本。 (base) C:\\Users\\Usercmake --versioncmake version 3.31.5CMake suite maintained and supported by Kitware (kitware.com/cmake). NinjaNinja 安装需要先安装 CMake 以及 Visual Studio，并配置好相应的环境变量。 Ninja 推荐的安装方式从源码编译安装，在 Windows 开始菜单中以管理员身份打开 “x64 Native Tools Command Prompt for VS 2019”，然后进入到 Ninja 文件夹目录，执行以下命令进行编译安装。 python configure.py --bootstrap 编译完毕后,产生的 ninja.exe 即为编译结果，可以将其置于需要的目录下，并添加到环境变量中。 根据下图所示，添加 ninja.exe 所在目录到环境变量中，并在终端检测是否成功。 ninja.exeninja: no work to do. 3D Vision 环境配置经过多次安装测试，总结经验后对该环境配置流程有比较清晰的认识，这主要用于在其他任何涉及 3D Vision 的项目中，在此基础的环境上可以快速复现项目环境。 下面是全部的安装包，主要包含依赖于 3DGS 的库以及 Pytorch3d。 Source CondaPip Library Version 3DGS Conda python 3.9 3DGS Conda cudatoolkit 11.3 3DGS Conda pytorch 1.12.1 3DGS Conda torchvision 0.13.1 3DGS Conda torchaudio 0.12.1 3DGS Conda plyfile 3DGS Conda tqdm 3DGS Pip opencv-python 3DGS Pip joblib 3DGS Pip submodulesdiff-gaussian-rasterization 3DGS Pip submodulessimple-knn 3DGS Pip submodulesfused_ssim Pytorch3d cub 1.11.0 Pytorch3d conda fvcore Pytorch3d conda iopath Pytorch3d pytorch3d 0.7.1 Step 1 创建虚拟环境# envconda create -n 3dv python=3.9conda activate 3dv Step 2 安装 CUDA Toolkit首先安装显卡驱动，一般通过 NVIDIA 应用程序进行安装。通过命令 nvidia-smi 可以查看显卡驱动版本以及最高可支持的 CUDA 版本。 选择的 CUDA 版本需要与 Pytorch 版本对应，可以在 CUDA Toolkit Archive 查询到对应版本并下载。 在这里我们选择的是 CUDA 11.3 的，下载选择的命令如下图所示： 下载完成后，按照提示进行安装，等待安装完成后，其会自动添加环境变量。 在命令行输入 nvcc --version 可以查看 CUDA 版本。成功安装后，将会显示如下图的版本信息： 当我们同时在 Windows 上安装了多个 CUDA 版本，并且希望能够通过指定环境变量进行切换。 Step 1: 修改系统环境变量 CUDA_PATH，将其改为目标 CUDA 版本对应的路径； Step 2: 修改 Path 中对应的 CUDA bin 和 libnvvp 两个路径，使得目标 CUDA 的这两个路径在所有 CUDA 版本的最上方。 Step 3 安装 Pytorch最好的安装方式就是对照 CUDA 版本，直接从 Pytorch 官网 INSTALLING PREVIOUS VERSIONS OF PYTORCH 中选择对应的安装命令进行安装。 # torchconda install pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=11.3 -c pytorch 这里的 cudatoolkit11.3 虽然能够在虚拟环境中作为一个安装包安装上（如下图所示），但是其不等于 CUDA Toolkit，仍需要 Step 2 中安装 CUDA Toolkit并配置好环境变量。 Step 4 安装 3DGS 的子模块一般在用到这些子模块的项目中，首先通过递归式地克隆项目仓库以使得包含对应的子模块，然后在项目环境中直接安装即可。 对虚拟环境的要求仅仅是 CUDA 版本和 Pytorch 版本对应起来，并且需要确保 CUDA Toolkit 也已经正确安装。 # submodulespip install submodules/diff-gaussian-rasterizationpip install submodules/simple-knn Step 5 安装 NVIDIA CUB从 NVIDIA CUB 根据 cuda 版本对应以下表格下载对应的 cub 版本(本机选择cub-1.11.0)，解压并设置环境变量 CUB_HOME。 CUB Release Included In 2.0.1 CUDA Toolkit 12.0 2.0.0 TBD 1.17.2 TBD 1.17.1 TBD 1.17.0 TBD 1.16.0 TBD 1.15.0 NVIDIA HPC SDK 22.1 CUDA Toolkit 11.6 1.14.0 NVIDIA HPC SDK 21.9 1.13.1 CUDA Toolkit 11.5 1.13.0 NVIDIA HPC SDK 21.7 1.12.1 CUDA Toolkit 11.4 1.12.0 NVIDIA HPC SDK 21.3 1.11.0 CUDA Toolkit 11.3 1.10.0 NVIDIA HPC SDK 20.9 CUDA Toolkit 11.2 1.9.10-1 NVIDIA HPC SDK 20.7 CUDA Toolkit 10.2 Step 6 安装 Pytorch3d 依赖项根据 Pytorch3d 官方文档，需要安装依赖项如下： conda install -c fvcore -c iopath -c conda-forge fvcore iopathconda install jupyterpip install scikit-image matplotlib imageio plotly opencv-pythonpip install black usort flake8 flake8-bugbear flake8-comprehensions Step 7 安装 Pytorch3d推荐的做法是通过源码编译安装，按照 Pytorch3d Installation 说明，从 Pytorch3d Release 中根据 Pytorch 的版本选择合适的 Pytorch3d 版本，并且尽量使得 Pytorch 版本处于 Pytorch3d 支持的版本范围中间。 选择下载 pytorch3d-0.7.2，解压后进入编译安装环节。 在 Windows 开始菜单中以管理员身份打开 x64 Native Tools Command Prompt for VS 2019，然后进入 pytorch3d 解压后的文件夹目录，配置 Visual Studio 环境变量。 set DISTUTILS_USE_SDK=1set PYTORCH3D_NO_NINJA=1 最后切换到 Anaconda 环境，进入 pytorch3d 文件夹的目录，在虚拟环境 Pytorch3d 下执行以下命令编译安装。 python setup.py install 最后直接执行命令，测试 Pytorch3d 是否安装成功。 import pytorch3d pytorch3d.__version__0.7.2 配置的环境依赖environment.ymlname: pytorch3dchannels: - pytorch - iopath - conda-forge - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2 - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - defaultsdependencies: - anyio=4.6.2=py39haa95532_0 - argon2-cffi=21.3.0=pyhd3eb1b0_0 - argon2-cffi-bindings=21.2.0=py39h827c3e9_1 - asttokens=2.0.5=pyhd3eb1b0_0 - async-lru=2.0.4=py39haa95532_0 - attrs=24.3.0=py39haa95532_0 - babel=2.16.0=py39haa95532_0 - backcall=0.2.0=pyhd3eb1b0_0 - beautifulsoup4=4.12.3=py39haa95532_0 - blas=1.0=mkl - bleach=6.2.0=py39haa95532_0 - brotli-python=1.0.9=py39h5da7b33_9 - ca-certificates=2025.1.31=h56e8100_0 - certifi=2025.1.31=py39haa95532_0 - cffi=1.17.1=py39h827c3e9_1 - charset-normalizer=3.3.2=pyhd3eb1b0_0 - colorama=0.4.6=pyhd8ed1ab_1 - comm=0.2.1=py39haa95532_0 - cudatoolkit=11.3.1=h59b6b97_2 - debugpy=1.8.11=py39h5da7b33_0 - decorator=5.1.1=pyhd3eb1b0_0 - defusedxml=0.7.1=pyhd3eb1b0_0 - exceptiongroup=1.2.0=py39haa95532_0 - executing=0.8.3=pyhd3eb1b0_0 - freetype=2.4.10=0 - fvcore=0.1.5.post20221221=pyhd8ed1ab_0 - h11=0.14.0=py39haa95532_0 - httpcore=1.0.2=py39haa95532_0 - httpx=0.27.0=py39haa95532_0 - icu=73.1=h6c2663c_0 - idna=3.7=py39haa95532_0 - importlib-metadata=8.5.0=py39haa95532_0 - importlib_metadata=8.5.0=hd3eb1b0_0 - intel-openmp=2023.1.0=h59b6b97_46320 - iopath=0.1.9=py39 - ipykernel=6.29.5=py39haa95532_1 - ipython=8.15.0=py39haa95532_0 - ipywidgets=8.1.5=py39haa95532_0 - jedi=0.19.2=py39haa95532_0 - jinja2=3.1.5=py39haa95532_0 - jpeg=9e=h827c3e9_3 - json5=0.9.25=py39haa95532_0 - jsonschema=4.23.0=py39haa95532_0 - jsonschema-specifications=2023.7.1=py39haa95532_0 - jupyter=1.1.1=py39haa95532_0 - jupyter-lsp=2.2.0=py39haa95532_0 - jupyter_client=8.6.3=py39haa95532_0 - jupyter_console=6.6.3=py39haa95532_0 - jupyter_core=5.7.2=py39haa95532_0 - jupyter_events=0.10.0=py39haa95532_0 - jupyter_server=2.14.1=py39haa95532_0 - jupyter_server_terminals=0.4.4=py39haa95532_1 - jupyterlab=4.3.4=py39haa95532_0 - jupyterlab_pygments=0.1.2=py_0 - jupyterlab_server=2.27.3=py39haa95532_0 - jupyterlab_widgets=3.0.13=py39haa95532_0 - krb5=1.20.1=h5b6d351_0 - lerc=4.0.0=h5da7b33_0 - libclang13=14.0.6=default_h8e68704_2 - libdeflate=1.22=h5bf469e_0 - libpng=1.6.39=h8cc25b3_0 - libpq=17.2=h70ee33d_0 - libsodium=1.0.18=h62dcd97_0 - libtiff=4.5.1=h44ae7cf_1 - libuv=1.48.0=h827c3e9_0 - libwebp=1.2.4=h2bbff1b_0 - libwebp-base=1.2.4=h2bbff1b_1 - lz4-c=1.9.4=h2bbff1b_1 - markupsafe=3.0.2=py39h827c3e9_0 - matplotlib-inline=0.1.6=py39haa95532_0 - mistune=2.0.4=py39haa95532_0 - mkl=2023.1.0=h6b88ed4_46358 - mkl-service=2.4.0=py39h827c3e9_2 - mkl_fft=1.3.11=py39h827c3e9_0 - mkl_random=1.2.8=py39hc64d2fc_0 - nbclient=0.8.0=py39haa95532_0 - nbconvert=7.16.4=py39haa95532_0 - nbformat=5.10.4=py39haa95532_0 - nest-asyncio=1.6.0=py39haa95532_0 - notebook=7.3.2=py39haa95532_0 - notebook-shim=0.2.3=py39haa95532_0 - openssl=3.4.1=ha4e3fda_0 - overrides=7.4.0=py39haa95532_0 - packaging=24.2=py39haa95532_0 - pandocfilters=1.5.0=pyhd3eb1b0_0 - parso=0.8.4=py39haa95532_0 - pickleshare=0.7.5=pyhd3eb1b0_1003 - pip=25.0=py39haa95532_0 - platformdirs=3.10.0=py39haa95532_0 - ply=3.11=py39haa95532_0 - portalocker=3.0.0=py39hcbf5309_0 - prometheus_client=0.21.0=py39haa95532_0 - prompt-toolkit=3.0.43=py39haa95532_0 - prompt_toolkit=3.0.43=hd3eb1b0_0 - psutil=5.9.0=py39h827c3e9_1 - pure_eval=0.2.2=pyhd3eb1b0_0 - pycparser=2.21=pyhd3eb1b0_0 - pygments=2.15.1=py39haa95532_1 - pyqt=5.15.10=py39h5da7b33_1 - pyqt5-sip=12.13.0=py39h827c3e9_1 - pysocks=1.7.1=py39haa95532_0 - python=3.9.21=h8205438_1 - python-dateutil=2.9.0post0=py39haa95532_2 - python-fastjsonschema=2.20.0=py39haa95532_0 - python-json-logger=3.2.1=py39haa95532_0 - python_abi=3.9=2_cp39 - pytorch=1.12.1=py3.9_cuda11.3_cudnn8_0 - pytorch-mutex=1.0=cuda - pywin32=307=py39ha51f57c_3 - pywinpty=2.0.14=py39h72d21ff_0 - pyyaml=6.0.2=py39hf73967f_2 - pyzmq=26.2.0=py39h5da7b33_0 - qt-main=5.15.2=h19c9488_12 - qtconsole=5.6.1=py39haa95532_0 - qtpy=2.4.1=py39haa95532_0 - referencing=0.30.2=py39haa95532_0 - requests=2.32.3=py39haa95532_1 - rfc3339-validator=0.1.4=py39haa95532_0 - rfc3986-validator=0.1.1=py39haa95532_0 - rpds-py=0.22.3=py39h636fa0f_0 - send2trash=1.8.2=py39haa95532_1 - setuptools=75.8.0=py39haa95532_0 - sip=6.7.12=py39h5da7b33_1 - six=1.16.0=pyhd3eb1b0_1 - sniffio=1.3.0=py39haa95532_0 - soupsieve=2.5=py39haa95532_0 - sqlite=3.45.3=h2bbff1b_0 - stack_data=0.2.0=pyhd3eb1b0_0 - tabulate=0.9.0=pyhd8ed1ab_2 - tbb=2021.8.0=h59b6b97_0 - termcolor=2.5.0=pyhd8ed1ab_1 - terminado=0.17.1=py39haa95532_0 - tinycss2=1.4.0=py39haa95532_0 - tk=8.6.14=h0416ee5_0 - tomli=2.0.1=py39haa95532_0 - torchaudio=0.12.1=py39_cu113 - torchvision=0.13.1=py39_cu113 - tornado=6.4.2=py39h827c3e9_0 - tqdm=4.67.1=pyhd8ed1ab_1 - traitlets=5.14.3=py39haa95532_0 - typing-extensions=4.12.2=py39haa95532_0 - typing_extensions=4.12.2=py39haa95532_0 - tzdata=2025a=h04d1e81_0 - ucrt=10.0.22621.0=h57928b3_1 - urllib3=2.3.0=py39haa95532_0 - vc=14.42=haa95532_4 - vc14_runtime=14.42.34433=h6356254_24 - vs2015_runtime=14.42.34433=hfef2bbc_24 - wcwidth=0.2.5=pyhd3eb1b0_0 - webencodings=0.5.1=py39haa95532_1 - websocket-client=1.8.0=py39haa95532_0 - wheel=0.45.1=py39haa95532_0 - widgetsnbextension=4.0.13=py39haa95532_0 - win_inet_pton=1.1.0=py39haa95532_0 - winpty=0.4.3=4 - xz=5.6.4=h4754444_1 - yacs=0.1.8=pyh29332c3_2 - yaml=0.2.5=h8ffe710_2 - zeromq=4.3.5=hd77b12b_0 - zipp=3.21.0=py39haa95532_0 - zlib=1.2.13=h8cc25b3_1 - zstd=1.5.6=h8880b57_0 - pip: - absl-py==2.1.0 - astunparse==1.6.3 - backgroundmattingv2==1.0 - black==25.1.0 - cachetools==5.5.1 - contourpy==1.2.1 - cython==3.0.12 - dearpygui==2.0.0 - diff-gaussian-rasterization==0.0.0 - dlib==19.24.6 - docstring-parser==0.16 - eval-type-backport==0.2.2 - face-detection-tflite==0.6.0 - ffmpeg-python==0.2.0 - flake8==7.1.2 - flake8-bugbear==24.12.12 - flake8-comprehensions==3.16.0 - flatbuffers==25.2.10 - future==1.0.0 - gast==0.4.0 - gdown==5.2.0 - google-auth==2.38.0 - google-auth-oauthlib==1.0.0 - google-pasta==0.2.0 - h5py==3.12.1 - imgaug==0.4.0 - jax==0.4.30 - jaxlib==0.4.30 - keras==2.12.0 - libclang==18.1.1 - libcst==1.6.0 - lxml==5.3.1 - matplotlib==3.8.0 - mccabe==0.7.0 - ml-dtypes==0.5.1 - moreorless==0.4.0 - mypy-extensions==1.0.0 - narwhals==1.26.0 - numpy==1.22.3 - nvdiffrast==0.3.3 - oauthlib==3.2.2 - opt-einsum==3.4.0 - pandas==2.0.3 - pathspec==0.12.1 - pillow==9.5.0 - plotly==6.0.0 - plyfile==1.1 - protobuf==4.25.6 - pyasn1==0.6.1 - pyasn1-modules==0.4.1 - pycodestyle==2.12.1 - pyflakes==3.2.0 - python-gflags==3.1.2 - pytorch3d==0.7.2 - requests-oauthlib==2.0.0 - roma==1.5.1 - rsa==4.9 - scikit-image==0.22.0 - scipy==1.11.4 - shapely==2.0.7 - shtab==1.7.1 - simple-knn==0.0.0 - star==0.1.0 - stdlibs==2024.12.3 - tensorboard==2.12.3 - tensorflow==2.12.0 - tensorflow-estimator==2.12.0 - tensorflow-intel==2.12.0 - tensorflow-io-gcs-filesystem==0.31.0 - terminaltables==3.1.10 - toml==0.10.2 - trailrunner==1.4.0 - typeguard==4.4.2 - tyro==0.9.14 - usort==1.0.8.post1 - vhap==0.0.2 - wrapt==1.14.1prefix: D:\\00MyWorkplace\\01anaconda\\envs\\pytorch3d","tags":["3D Vision","Windows"],"categories":["blog"]},{"path":"/KLBook/more/index.html","content":"关于博主关于本站我的歌单我的游戏 人格类型 测试结果来自 16Personalities。 我的游戏原神 鸣潮"},{"path":"/KLBook/news/index.html","content":"2025 2025-07-19周日骑车去公园🏞️2025-07-15偶遇！梦幻的的紫色夕阳✨"},{"path":"/KLBook/more/about/index.html","content":"关于博主关于本站我的歌单我的游戏"},{"path":"/KLBook/more/games/index.html","content":"关于博主关于本站我的歌单我的游戏 原神 个人数据 鸣潮"},{"path":"/KLBook/more/music/index.html","content":"关于博主关于本站我的歌单我的游戏"},{"title":"Literature Taxonomy","path":"/KLBook/wiki/3DTalkingHead/index.html","content":"Motion-based Literature Structure基于运动表征的文献组织分类方法，主要从音频驱动肖像生成的现有方法进行分类，将文献按照不同运动表征方式分为基于顶点的（Vertex-based）和基于参数的（Parameter-based）两大类。 Vertex-based基于顶点的方法主要包括 Mesh、Keypoints 类别，通过直接从音频特征预测顶点位置或基于标准模板的顶点偏移量来实现运动的生成。 Mesh-based基于网格的表示方法通过顶点 (Vertex) 和面片 (Face) 构建数字人脸拓扑，实现精细形变控制。 局部区域的精细控制相比于Blendshape类方法，能够通过预测顶点坐标，细化口型内部区域，实现精细粒度的运动控制。 直观的运动表示通过显式几何表征人脸，易于构建顶点位移直接表示区域的运动特征，便于头部的运动生成。 Keypoints-based关键点表示通过稀疏的关键点来描述面部动作，并构建轻量级的驱动框架。 轻量驱动控制面向实时应用，计算量小，驱动效率高。 难以精细控制基于关键点的方法难以捕获微妙的面部运动，包括眼睛以及睫毛运动。 隐式关键点灵活性提取隐式关键点作为人脸表征，不依赖固定的3DMM顶点位置，维持效率的同时增加了驱动的灵活性。 Parameter-based基于参数的方法主要利用隐式的表征参数，进一步按照预定的规则进一步地合成参数化人脸。常用的表征方法包括，3DMM系数、NeRF隐式神经表征、3DGS显式表征方法，通过预测各个表征参数以及参数的变化量生成人脸运动。 3DMM Coefficients3DMM通过线性基向量组合构建参数化人脸模型，已成为学术研究中的主流范式。 形状混合稳定性通过参数化人脸驱动的方式，能够弱化中间系数预测损失更具稳定性。 显式参数可控性易于解耦出头部姿势与表情系数表征人脸，能够提高对人脸驱动的可控性并易于接受用户控制信号进行调控。 提供宏观整体结构基于混合形状的3DMM能够为驱动人脸提供很好的宏观结构，增强了全局表情的驱动生成。 NeRF-basedNeRF通过体积渲染构建稠密的辐射场表示，实现高质量人脸重建与合成，但渲染速度较慢，难以满足实时驱动需求。 较长的训练和推理时间需要数个小时完成一个角色的训练，并且推理时间缓慢，难以泛化到其他人物。 可控性差由于统一的隐式神经表示，难以接受多条件输入控制生成驱动视频；不能够显式地控制人脸表情、姿势，可能会造成不自然的结果。 复杂的网络结构需要隐式地学习庞大的MLP编码器，会进一步地限制收敛性和重建质量。 3DGS-based3D高斯点云通过各向异性高斯建模，实现了高质量实时渲染，通过引导高斯点分布实现人脸驱动。 高效驱动能力基于3DGS表征人脸，能够实现高效地人脸驱动，并借助渲染能力实现实时驱动，网络简单，计算高效。 泛化性弱借助高斯点的属性偏移实现驱动，依赖于人物的特定数据，难以实现跨人物之间的驱动。 Problem-based Literature Structure在统一的研究问题下，不同的研究工作侧重于解决统一研究问题的不同方面，因此需要对所有文献按照不同研究问题进行细化分类。 如图所示，将 Talking Head Synthesis 研究分为五个需求，分别是训练数据、口型同步、表情生成、头部姿势以及驱动效率。 对于不同的需求，分为更细化的研究目标，例如对数据集的要求需要是大规模、高质量数据，现有数据集有些虽然规模庞大，但是质量不高，相反质量较高的数据集，数量反而较少，这就造成了现有研究的矛盾； 口型同步 主要是对于音频和口型实现准确性与同步性，口型应当准确地对应当前音频要素的内容，并且能够与音频保持同步性； 表情生成 分为情感驱动和语境驱动两种方式，不同的驱动方式具有独特的影响； 头部姿势 是个性化风格，需要从大量数据中学习出多样的风格，需要尽可能地自然； 驱动效率 是实现模型紧凑型以及实时驱动的要求，一般模型参数量少、结构简单，便可以实现较高的驱动效率。 不同的工作，关注点不同，从数据集构造到实现高效驱动，涉及了不同的研究目标。 Model-based Literature StructureRegressive Models 精确度高直接学习最终的驱动结果，可控性高。 回归均值问题对于相同的音素会对应多种合理的表情变化，这种一对多的模糊性会造成回归模型过度产生过度平滑的人脸运动。 泛化性弱现有回归方法都是身份依赖型的，在零样本推理中表现出不自然的结果，一些方法需要额外引入适应网络进行个性化匹配。 Generative ModelsStrengths 生成多样化生成模型直接学习条件概率分布，建模潜在的数据分布，确保了多样的样本生成。 高保真驱动细节生成模型例如扩散模型能够捕获复杂的数据分布，隐式地记忆细微的个性化细节。 跨模态对齐生成模型尤其是Transformer结构能够将不同模态的特征进行高效融合，有效地建模音频复杂的解耦特征与运动序列之间的对应关系。 Shortcomings 跨模态映射的不确定性生成模型能够产生多样的数据样本，但是多样的数据分布导致驱动不稳定性。 多样且大规模数据需求生成模型的训练需要大规模的数据集学习潜在的数据分布。 训练和推理缓慢大规模生成模型面临着庞大的参数学习任务，需要大量的计算资源进行训练以及表现出缓慢的推理速度。"},{"title":"[ECCV 2020 3DGS] [MLP]","path":"/KLBook/wiki/human/solid-work/3DGS.html","content":"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis 研究框架"},{"title":"[ECCV 2020 NeRF] [MLP]","path":"/KLBook/wiki/human/solid-work/NeRF.html","content":"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis 研究框架"},{"title":"[ECCV 2020 3DGS] [MLP]","path":"/KLBook/wiki/scene/solid-work/3DGS.html","content":"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis 研究框架"},{"title":"[ECCV 2020 NeRF] [MLP]","path":"/KLBook/wiki/scene/solid-work/NeRF.html","content":"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis 研究框架"},{"title":"Generative Models","path":"/KLBook/wiki/3DTalkingHead/mesh-based/generative/index.html","content":"[CVPR 2023 CodeTalker] [Lip Sync.] [Context Expression]CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior Figure: CodeTalker (CVPR 2023) Task: Reduce cross-modal mapping uncertainty and regression-to-mean by casting speech-driven animation as a code query task.Motivation:By mapping speech to a finite proxy space (discrete codebook), uncertainty is reduced and synthesis quality improves.Motion: Mesh Vertex Offsets.Dataset: BIWI: 40 shared sentences across speakers. VOCASET: 255 unique sentences. Views: Uses VQ-VAE to learn a discrete code space, offering more detailed representations than parameter-based methods.Problems: Diversity: No explicit stochasticity declared; diversity evaluation lacking. Controllability: Lacks explicit emotion control; adjusting output requires vertex-level manipulation. Style: Stores stylized motion priors in a codebook — storing expressive priors remains under-explored. [ICCV 2025 GaussianSpeech] [Lip Sync.] [Context Expression]GaussianSpeech: Audio-Driven Gaussian Avatars Figure: GaussianSpeech (ICCV 2025) Task: Decomposes lip and wrinkle features from audio to animate the FLAME mesh; rendering via 3D Gaussian Splatting.Motivation: Real-time, free-viewpoint, photorealistic 3D-consistent facial synthesis.Motion: Mesh Vertex Offsets.Dataset: Multi-View Audio-Visual DatasetViews: Uses 3D Gaussian fields + dynamic deformation for lip-sync realism and flexibility in avatars. [IJCAI 2025 GLDiTalker] [Lip Sync.] [Context Expression]Speech-Driven 3D Facial Animation with Graph Latent Diffusion Transformer Figure: GLDiTalker (IJCAI 2025) Task: Improve lip-sync via graph-based encoder codebook; enhance diversity using latent diffusion.Motivation: Address modality inconsistencies and misalignment.Motion: Mesh Vertex Offsets.Dataset: BIWI: 40 shared sentences. VOCASET: 255 unique sentences. Views: Uses facial meshes and a graph-latent-diffusion transformer to enable temporally stable 3D facial animation."},{"title":"Regressive Models","path":"/KLBook/wiki/3DTalkingHead/mesh-based/regressive/index.html","content":"[TOG 2017 ADFA] [Lip Sync.] [Context Expression]Audio-Driven Facial Animation by Joint End-to-End Learning of Pose and Emotion Figure: ADFA (TOG 2017) Task: Using CNN Network to predict the vertex position of 3D Mesh by end-to-end learning.Motion: Mesh Vertex.Dataset: DI4D PRO system.Views: It maps the raw audio waveform to the 3D coordinates of a face mesh. A trainable parameter is defined to capture emotions. During inference, this parameter is modified to simulate different emotions.Problems: It captures the idiosyncrasies of an individual, making it inappropriate for generalization across characters. [CVPR 2019 VOCA] [Lip Sync.] [Context Expression]Capture, Learning, and Synthesis of 3D Speaking Styles Figure: VOCA (CVPR 2019) Task: Construct the encoder-decoder based on DNN and decompose the identity from audio to achieve generalization.Motion: Mesh Vertex Offsets.Dataset: VOCASET, 4D Scanned Mesh data.Views: It is an end-to-end deep neural network for speech-to-animation translation trained on multiple subjects. From extracted audio embedding, VOCA regresses 3D vertices on a FLAME face model conditioned on a subject label.Problems: Requires high quality 4D scans recorded in a studio setup. Needs more training data due to latent modalities (prior parametric models). [ECCV 2024 UniTalker] [Dataset Assembly]UniTalker: Scaling up Audio-Driven 3D Facial Animation through A Unified Model Figure: UniTalker learns from various data formats and scales up training data (ECCV 2024) Figure: UniTalker architecture with multi-head decoders (ECCV 2024) Task: Existing 3D dataset annotation inconsistency restricts previous models to specific data formats, limiting scalability.Motivation: Unified 5 public datasets + 3 new ones, expanding training data from 1 hour to 18.5 hours. Trains multi-head decoders to supervise multiple representation targets.Motion: For vertex-based annotations, motion is vertex displacement; for parameter-based annotations, motion is parameter vectors. [MM 2020 Learn2Talk] [Lip Sync.] [3D Supervision Form 2D]Learn2Talk: 3D Talking Face Learns from 2D Talking Face Figure: Learn2Talk (MM 2020) Task: Enhances 3D lip-sync accuracy by extending SyncNet to SyncNet3D.Motivation: SyncNet exists in 2D talking heads to measure speech-motion temporal relationships.Summary: LVE: The 3D lip vertex error (LVE) as a 3D reconstruction loss represents per-frame 3D accuracy.SyncNet3D: Measures the temporal relationship between speech audio and facial motion."},{"title":"Generative Models","path":"/KLBook/wiki/3DTalkingHead/keypoints-based/generative/index.html","content":"[ICLR 2023 GeneFace] [Lip Sync.] [General Animation]GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis Figure: GeneFace (ICLR 2023) Task: Select 68 facial keypoints and predict the offset of keypoints to animate the NeRF rendering.Motivation: The generalization is limited by the small scale of training data.Motion: Keypoints Offset.Dataset: LRS3-TEDViews: It attempts to reduce NeRF artifacts by translating speech features into facial landmarks, but this often results in inaccurate lip movements. It’s also hard to reproduce actions such as blinking and eyebrow-raising. [arXiv 2025 KDTalker] [Lip Sync.] [Context Expression] [Head Pose]Unlock Pose Diversity: Accurate and Efficient Implicit Keypoint-based Spatiotemporal Diffusion for Audio-driven Talking Portrait Figure: KDTalker (arXiv 2025) Task: Integrate unsupervised 3D keypoints (K) with diffusion models.Motivation: Fixed nature of 3DMM keypoints without flexibility.Motion: Keypoints Position.Dataset: Training: VoxCeleb Evaluation: HDTF"}]